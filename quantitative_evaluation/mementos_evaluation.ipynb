{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai, json, tqdm\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = ## YOUR OPENAI API KEY HERE\n",
    "openai.api_key = api_key\n",
    "\n",
    "def chat_classify(gt, pred, model: str = \"gpt-3.5-turbo\"):\n",
    "    def request_openai_api():\n",
    "        #===============\n",
    "        messages = [\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": f\"\"\"I will provide you two paragraphs. The first paragraph is human-composed and the second paragraph is generated by an AI model. I want to evaluate the hallucination in the second paragraph. You will extract the object and action words or phrases from the following text. The objects should have a tangible meaning and consist of no more than two words; non-tangible objects. or objects irrelevant to the actions, should not be extracted. The action words or phrases should only relate to the extracted objects. Also, you must convert the corresponding actions to their complete root form. Then, for the final answer, please generate 4 lists and must transfer the synonyms in 4 lists into the same word. Please directly output the final object and action lists in two paragraphs, respectively as in the form in the example below without any justifications or intermediate steps.\n",
    "             \n",
    "             Here is an example:\n",
    "             1. The video captures a dog's cautious interaction with a metal toy inside a house. The dog appears wary and maintains a distance from the unfamiliar object, barking to express its disapproval and possibly intimidation. As the toy moves, the dog's reaction is to bark and lean backward, showing a clear sign of being unsettled by the toy's motion. When the toy momentarily ceases movement, the dog also stops, remaining alert and attentive. At the end of the image, when the toy comes to a halt, the dog looks up, still processing the strange encounter with the inanimate object.\n",
    "             2. The video is a collage of multiple pictures featuring two dogs playing with a toy alligator. The dogs are in various positions, with some of them standing on the toy alligator, while others are interacting with it in different ways. The collage captures the dogs' playfulness and excitement as they engage with the toy alligator. \n",
    "             The lists are:\n",
    "             Object list 1: [dog, toy, house] \n",
    "             Action list 1: [interaction, bark, express intimidation, move, lean backward, stop, look up] \n",
    "             Object list 2: [dog, toy] \n",
    "             Action list 2: [play, stand, interaction] \n",
    "             Here are the paragraphs for which you should generate object and action lists:\n",
    "\n",
    "             1. {str(gt)}\n",
    "             2. {str(pred)}\n",
    "\n",
    "             Remember, The objects should have a tangible meaning and consist of no more than two words; non-tangible objects should not be extracted. The action words or phrases should only relate to the extracted objects. The object and action lists for each paragraph are (only give 2 object lists and 2 action lists; NOTHING MORE NOTHING LESS):\"\"\"},\n",
    "        ]   \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "        )\n",
    "        return response\n",
    "    return request_openai_api()\n",
    "\n",
    "def get_obj_action_lists(llm_output):\n",
    "    \"\"\"\n",
    "    First paragraph is the ground truth and second paragraph is the LLM generated description.\n",
    "\n",
    "    Returns:\n",
    "    object_lists: List of object lists extracted from the LLM output (should be length 2)\n",
    "    action_lists: List of action lists extracted from the LLM output (should be length 2)\n",
    "    \"\"\"\n",
    "    object_list_pattern = r\"object list (\\d+): \\[(.*)\\]\"\n",
    "    action_list_pattern = r\"action list (\\d+): \\[(.*)\\]\"\n",
    "    object_lists = re.findall(object_list_pattern, llm_output.lower())\n",
    "    action_lists = re.findall(action_list_pattern, llm_output.lower())\n",
    "\n",
    "    object_lists = [x[1].split(\", \") for x in object_lists]\n",
    "    action_lists = [x[1].split(\", \") for x in action_lists]\n",
    "\n",
    "    return object_lists, action_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = ## PATH TO YOUR JSON FILE\n",
    "json_name = json_path.split('/')[-1].replace('.json', '')\n",
    "data = json.load(open(json_path))\n",
    "data_eval_lists = {}\n",
    "\n",
    "for content in tqdm.tqdm(data, total=len(data)):\n",
    "    vid_name = content['video_name'].replace('.mp4', '')\n",
    "    vid_gt_desc = content['A']\n",
    "    vid_pred_desc = content['pred']\n",
    "\n",
    "    try:\n",
    "        response = chat_classify(vid_gt_desc, vid_pred_desc).choices[0].message['content']\n",
    "\n",
    "        object_lists, action_lists = get_obj_action_lists(response)\n",
    "    except:\n",
    "        object_lists, action_lists = [], []\n",
    "\n",
    "    # check if the extraction was successful, if not then retry\n",
    "    if len(object_lists) != 2 or len(action_lists) != 2:\n",
    "        # retry 3 more times, if still not successful, then skip\n",
    "        for i in range(3):\n",
    "            try:\n",
    "                response = chat_classify(vid_gt_desc, vid_pred_desc).choices[0].message['content']\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            object_lists, action_lists = get_obj_action_lists(response)\n",
    "            if len(object_lists) == 2 and len(action_lists) == 2:\n",
    "                break\n",
    "            \n",
    "            object_lists, action_lists = 'Error in extracting', 'Error in extracting'\n",
    "            print(f'\\tError in extracting for {vid_name}. Retry {i+1}/3')\n",
    "\n",
    "            if i == 2:\n",
    "                print(f'\\t\\t{response}')\n",
    "\n",
    "    data_eval_lists[vid_name] = {}\n",
    "    data_eval_lists[vid_name]['gt'] = vid_gt_desc\n",
    "    data_eval_lists[vid_name]['pred'] = vid_pred_desc\n",
    "    data_eval_lists[vid_name]['llm_response'] = response\n",
    "    data_eval_lists[vid_name]['object_lists'] = object_lists\n",
    "    data_eval_lists[vid_name]['action_lists'] = action_lists\n",
    "\n",
    "# save\n",
    "json.dump(data_eval_lists, open(f'./OBJ-ACTIONS_{json_name}.json', 'w'), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_path = f'./OBJ-ACTIONS_{json_name}.json'\n",
    "oa_data = json.load(open(j_path))\n",
    "\n",
    "a_re = []\n",
    "a_pre = []\n",
    "a_f1 = []\n",
    "\n",
    "o_re = []\n",
    "o_pre = []\n",
    "o_f1 = []\n",
    "\n",
    "for vid in oa_data:\n",
    "    if len(oa_data[vid]['action_lists']) == 0:\n",
    "        continue\n",
    "    # Action metrics\n",
    "    a_reference_list, a_pred_list = oa_data[vid]['action_lists'][0], oa_data[vid]['action_lists'][1]\n",
    "\n",
    "    a_tp = len(set(a_reference_list) & set(a_pred_list))\n",
    "    a_fp = len(set(a_pred_list) - set(a_reference_list))\n",
    "    a_fn = len(set(a_reference_list) - set(a_pred_list))\n",
    "\n",
    "    a_recall = a_tp / (a_tp + a_fn) if (a_tp + a_fn) != 0 else 0\n",
    "    a_precision = a_tp / (a_tp + a_fp) if (a_tp + a_fp) != 0 else 0\n",
    "    a_f1_score = 2 * (a_precision * a_recall) / (a_precision + a_recall) if (a_precision + a_recall) != 0 else 0\n",
    "\n",
    "    a_re.append(a_recall)\n",
    "    a_pre.append(a_precision)\n",
    "    a_f1.append(a_f1_score)\n",
    "\n",
    "    # Object metrics\n",
    "    o_reference_list, o_pred_list = oa_data[vid]['object_lists'][0], oa_data[vid]['object_lists'][1]\n",
    "\n",
    "    o_tp = len(set(o_reference_list) & set(o_pred_list))\n",
    "    o_fp = len(set(o_pred_list) - set(o_reference_list))\n",
    "    o_fn = len(set(o_reference_list) - set(o_pred_list))\n",
    "\n",
    "    o_recall = o_tp / (o_tp + o_fn) if (o_tp + o_fn) != 0 else 0\n",
    "    o_precision = o_tp / (o_tp + o_fp) if (o_tp + o_fp) != 0 else 0\n",
    "    o_f1_score = 2 * (o_precision * o_recall) / (o_precision + o_recall) if (o_precision + o_recall) != 0 else 0\n",
    "\n",
    "    o_re.append(o_recall)\n",
    "    o_pre.append(o_precision)\n",
    "    o_f1.append(o_f1_score)\n",
    "\n",
    "# print metrics\n",
    "print(j_path)\n",
    "print('Object Precision/Recall/F1')\n",
    "print(f'{np.mean(o_re):.4f} / {np.mean(o_pre):.4f} / {np.mean(o_f1):.4f}')\n",
    "\n",
    "print('Action Precision/Recall/F1')\n",
    "print(f'{np.mean(a_re):.4f} / {np.mean(a_pre):.4f} / {np.mean(a_f1):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the JSON file\n",
    "os.remove(j_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
