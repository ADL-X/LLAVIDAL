# LLAVIDAL
#  LLAVIDAL üèÉüë©‚Äçü¶Ø‚Äç‚û°Ô∏èüó®Ô∏è

### LLAVIDAL: Towards Detailed Video Understanding via Large Vision and Language Models

#### [Rajatsubhra Chakraborty](https://chakrabortyrajatsubhra.github.io)<sup>1</sup>* , [Arkaprava Sinha](https://www.linkedin.com/in/arkaprava-sinha)<sup>1</sup>* , [Dominick Reilly](https://dominick-reilly.github.io/)<sup>1</sup>* , [Manish Kumar Govind](https://sites.google.com/view/manishkumargovind/home)<sup>1</sup>, [Pu Wang](https://webpages.charlotte.edu/pwang13/)<sup>1</sup>,[Francois Bremond](http://www-sop.inria.fr/members/Francois.Bremond/)<sup>2</sup> and [Srijan Das](https://srijandas07.github.io)<sup>1</sup>
\* Equally contributing first authors

##### Affiliations:
<sup>1</sup> University of North Carolina at Charlotte  
<sup>2</sup> INRIA, Universit√© C√¥te d‚ÄôAzur


## :loudspeaker: Latest Updates
Placeholder for updates

---

## Online Demo :computer:

Placeholder for online demo description

---

## LLAVIDAL Overview :bulb:

Placeholder for LLAVIDAL overview

---

## Contributions ‚≠ê:

We introduce ADL-X, the first multiview RGBD instruction ADL dataset, curated through a
novel semi-automated framework for training LLVMs.

‚Ä¢ LLAVIDAL is introduced as the first LLVM tailored for ADL, incorporating 3D poses and
object cues into the embedding space of the LLM.

‚Ä¢ A new benchmark, ADLMCQ, is proposed for an objective evaluation of LLVMs on ADL
tasks, featuring MCQ tasks for action recognition & forecasting.

‚Ä¢ Exhaustive experiments are conducted to determine the optimal strategy for integrating
poses or objects into LLAVIDAL. Evaluation of existing LLVMs on ADLMCQ and video
description tasks reveals that LLAVIDAL trained on ADL-X significantly outperforms
baseline LLVMs

---

## Installation :wrench:

Placeholder for installation instructions

---

## Running Demo Offline :cd:

Placeholder for running demo offline instructions

---

## Training :train:

Placeholder for training instructions

---

## Video Instruction Dataset :open_file_folder:

Placeholder for video instruction dataset description

---

## Quantitative Evaluation :bar_chart:

Placeholder for quantitative evaluation description

---

## Qualitative Analysis :mag:

Placeholder for qualitative analysis description

---

## Acknowledgements :pray:

+ [LLaMA](https://github.com/facebookresearch/llama): A great attempt towards open and efficient LLMs!
+ Additional acknowledgements as needed.

If you're using LLAVIDAL in your research or applications, please cite using this BibTeX:
```bibtex
@inproceedings{Chakraborty2024LLAVIDAL,
    title={LLAVIDAL: Benchmarking Large LAnguage VIsion Models for Daily Activities of Living},
    author={Chakraborty, Rajatsubhra and Sinha, Arkaprava and Reilly, Dominick and Govind, Manish Kumar and Wang, Pu and Bremond, Fran√ßois and Das, Srijan},
    booktitle={},
    year={2024}
}
